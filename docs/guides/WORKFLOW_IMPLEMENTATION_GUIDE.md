# å·¥ä½œæµç¨‹å®žçŽ°æŒ‡å—

## ðŸ“‹ æ¦‚è¿°

åŸºäºŽç”¨æˆ·æä¾›çš„ç»†åŒ–å·¥ä½œæµç¨‹å›¾ï¼Œæœ¬æ–‡æ¡£æä¾›äº†å…·ä½“çš„å®žçŽ°æŒ‡å—ï¼ŒåŒ…æ‹¬ä»£ç ç¤ºä¾‹ã€é…ç½®è¯´æ˜Žå’Œæœ€ä½³å®žè·µã€‚

## ðŸ”„ å®Œæ•´å·¥ä½œæµç¨‹å®žçŽ°

### ä¸»æŽ§åˆ¶å™¨å®žçŽ°
```python
# controllers/enterprise_workflow_controller.py
from fastapi import APIRouter, HTTPException, Depends
from typing import Dict, Any, Optional
from datetime import datetime
import asyncio
import logging

from services.enterprise_service import EnterpriseService
from services.parallel_task_orchestrator import ParallelTaskOrchestrator
from services.new_enterprise_discovery import NewEnterpriseDiscoveryService
from utils.workflow_logger import WorkflowLogger

logger = logging.getLogger(__name__)
router = APIRouter(prefix="/api/v1/workflow", tags=["ä¼ä¸šä¿¡æ¯å·¥ä½œæµ"])

class EnterpriseWorkflowController:
    def __init__(self):
        self.enterprise_service = EnterpriseService()
        self.orchestrator = ParallelTaskOrchestrator()
        self.discovery_service = NewEnterpriseDiscoveryService()
        self.workflow_logger = WorkflowLogger()

@router.post("/process")
async def process_enterprise_workflow(
    enterprise_name: str,
    force_refresh: bool = False
) -> Dict[str, Any]:
    """
    ä¼ä¸šä¿¡æ¯å¤„ç†å·¥ä½œæµç¨‹ä¸»å…¥å£
    """
    controller = EnterpriseWorkflowController()
    
    try:
        # è®°å½•å·¥ä½œæµå¼€å§‹
        workflow_id = await controller.workflow_logger.start_workflow(enterprise_name)
        
        # æ‰§è¡Œå®Œæ•´å·¥ä½œæµç¨‹
        result = await controller.execute_complete_workflow(
            enterprise_name, 
            workflow_id, 
            force_refresh
        )
        
        # è®°å½•å·¥ä½œæµå®Œæˆ
        await controller.workflow_logger.complete_workflow(workflow_id, result)
        
        return {
            "workflow_id": workflow_id,
            "status": "success",
            "result": result,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"å·¥ä½œæµç¨‹æ‰§è¡Œå¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"å·¥ä½œæµç¨‹æ‰§è¡Œå¤±è´¥: {str(e)}")

class EnterpriseWorkflowController:
    async def execute_complete_workflow(self, enterprise_name: str, 
                                      workflow_id: str, 
                                      force_refresh: bool = False) -> Dict[str, Any]:
        """
        æ‰§è¡Œå®Œæ•´çš„ä¼ä¸šä¿¡æ¯å¤„ç†å·¥ä½œæµç¨‹
        """
        workflow_context = {
            "workflow_id": workflow_id,
            "enterprise_name": enterprise_name,
            "start_time": datetime.now(),
            "force_refresh": force_refresh,
            "steps": []
        }
        
        try:
            # æ­¥éª¤1: åœ¨æœ¬åœ°DBæœç´¢ä¼ä¸šä¸»ä½“è®°å½•
            search_result = await self._step1_search_local_enterprise(
                enterprise_name, workflow_context
            )
            
            if not search_result["found"]:
                # è®°å½•æœªæ‰¾åˆ°ä¼ä¸š
                workflow_context["steps"].append({
                    "step": "enterprise_not_found",
                    "timestamp": datetime.now().isoformat(),
                    "message": "æœ¬åœ°æ•°æ®åº“ä¸­æœªæ‰¾åˆ°ä¼ä¸šè®°å½•"
                })
                
                # æ‰§è¡Œã€æ–°ä¼ä¸šå‘çŽ°ã€‘æµç¨‹
                discovery_result = await self._execute_new_enterprise_discovery(
                    enterprise_name, workflow_context
                )
                
                return {
                    "workflow_type": "new_enterprise_discovery",
                    "enterprise_name": enterprise_name,
                    "result": discovery_result,
                    "workflow_context": workflow_context
                }
            
            # ä¼ä¸šå­˜åœ¨ï¼Œæ‰§è¡Œå¹¶è¡Œå¤„ç†æµç¨‹
            enterprise_id = search_result["enterprise_id"]
            
            # é˜¶æ®µä¸€: å¹¶è¡Œä»»åŠ¡åˆ†å‘ä¸Žæ‰§è¡Œ
            stage1_result = await self._execute_stage1_parallel_processing(
                enterprise_id, enterprise_name, workflow_context
            )
            
            # é˜¶æ®µäºŒ: åŒæ­¥ç­‰å¾…ä¸Žä¾èµ–åˆ†æž
            stage2_result = await self._execute_stage2_dependency_analysis(
                stage1_result, workflow_context
            )
            
            # é˜¶æ®µä¸‰: ç»“æžœæ±‡æ€»
            final_result = await self._execute_stage3_result_integration(
                stage1_result, stage2_result, workflow_context
            )
            
            return {
                "workflow_type": "existing_enterprise_processing",
                "enterprise_id": enterprise_id,
                "enterprise_name": enterprise_name,
                "stage1_result": stage1_result,
                "stage2_result": stage2_result,
                "final_result": final_result,
                "workflow_context": workflow_context
            }
            
        except Exception as e:
            workflow_context["error"] = str(e)
            workflow_context["status"] = "failed"
            raise

    async def _step1_search_local_enterprise(self, enterprise_name: str, 
                                           context: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ­¥éª¤1: åœ¨æœ¬åœ°DBæœç´¢ä¼ä¸šä¸»ä½“è®°å½•
        """
        step_start = datetime.now()
        
        try:
            # æœç´¢æœ¬åœ°ä¼ä¸šè®°å½•
            enterprises = await self.enterprise_service.search_enterprises_by_name(
                enterprise_name, exact_match=True
            )
            
            if enterprises:
                # æ‰¾åˆ°ä¼ä¸šè®°å½•
                enterprise = enterprises[0]  # å–ç¬¬ä¸€ä¸ªåŒ¹é…ç»“æžœ
                
                step_result = {
                    "step": "search_local_enterprise",
                    "found": True,
                    "enterprise_id": enterprise["id"],
                    "enterprise_data": enterprise,
                    "execution_time": (datetime.now() - step_start).total_seconds(),
                    "timestamp": datetime.now().isoformat()
                }
            else:
                # æœªæ‰¾åˆ°ä¼ä¸šè®°å½•
                step_result = {
                    "step": "search_local_enterprise",
                    "found": False,
                    "enterprise_id": None,
                    "enterprise_data": None,
                    "execution_time": (datetime.now() - step_start).total_seconds(),
                    "timestamp": datetime.now().isoformat()
                }
            
            context["steps"].append(step_result)
            return step_result
            
        except Exception as e:
            step_result = {
                "step": "search_local_enterprise",
                "found": False,
                "error": str(e),
                "execution_time": (datetime.now() - step_start).total_seconds(),
                "timestamp": datetime.now().isoformat()
            }
            context["steps"].append(step_result)
            raise

    async def _execute_new_enterprise_discovery(self, enterprise_name: str, 
                                              context: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ‰§è¡Œã€æ–°ä¼ä¸šå‘çŽ°ã€‘æµç¨‹ - å…¨é¢è”ç½‘æœç´¢
        """
        step_start = datetime.now()
        
        try:
            # ä½¿ç”¨æ–°ä¼ä¸šå‘çŽ°æœåŠ¡è¿›è¡Œå…¨é¢æœç´¢
            discovery_result = await self.discovery_service.discover_new_enterprise(
                enterprise_name
            )
            
            step_result = {
                "step": "new_enterprise_discovery",
                "status": "completed",
                "discovery_data": discovery_result,
                "execution_time": (datetime.now() - step_start).total_seconds(),
                "timestamp": datetime.now().isoformat()
            }
            
            context["steps"].append(step_result)
            return step_result
            
        except Exception as e:
            step_result = {
                "step": "new_enterprise_discovery",
                "status": "failed",
                "error": str(e),
                "execution_time": (datetime.now() - step_start).total_seconds(),
                "timestamp": datetime.now().isoformat()
            }
            context["steps"].append(step_result)
            raise

    async def _execute_stage1_parallel_processing(self, enterprise_id: int, 
                                                enterprise_name: str,
                                                context: Dict[str, Any]) -> Dict[str, Any]:
        """
        é˜¶æ®µä¸€: å¹¶è¡Œä»»åŠ¡åˆ†å‘ä¸Žæ‰§è¡Œ
        """
        stage_start = datetime.now()
        
        try:
            # ä½¿ç”¨ä»»åŠ¡ç¼–æŽ’å™¨æ‰§è¡Œ6ä¸ªå¹¶è¡Œæ³³é“
            stage1_result = await self.orchestrator.execute_parallel_lanes(
                enterprise_id, enterprise_name, context.get("force_refresh", False)
            )
            
            step_result = {
                "step": "stage1_parallel_processing",
                "status": "completed",
                "lane_results": stage1_result,
                "execution_time": (datetime.now() - stage_start).total_seconds(),
                "timestamp": datetime.now().isoformat()
            }
            
            context["steps"].append(step_result)
            return stage1_result
            
        except Exception as e:
            step_result = {
                "step": "stage1_parallel_processing",
                "status": "failed",
                "error": str(e),
                "execution_time": (datetime.now() - stage_start).total_seconds(),
                "timestamp": datetime.now().isoformat()
            }
            context["steps"].append(step_result)
            raise

    async def _execute_stage2_dependency_analysis(self, stage1_result: Dict[str, Any],
                                                context: Dict[str, Any]) -> Dict[str, Any]:
        """
        é˜¶æ®µäºŒ: åŒæ­¥ç­‰å¾…ä¸Žä¾èµ–åˆ†æž
        """
        stage_start = datetime.now()
        
        try:
            # ç­‰å¾…æ³³é“Aå’Œæ³³é“Bå®Œæˆï¼ŒèŽ·å–åŒºåŸŸä¿¡æ¯å’Œäº§ä¸šä¿¡æ¯
            lane_a_result = stage1_result.get("lane_a")
            lane_b_result = stage1_result.get("lane_b")
            
            if (lane_a_result and lane_a_result.get("status") == "completed" and
                lane_b_result and lane_b_result.get("status") == "completed"):
                
                # æ‰§è¡Œæœ¬åœ°äº§ä¸šå¤§è„‘ä¸Žäº§ä¸šé“¾å®šä½åˆ†æž
                ecosystem_analysis = await self.orchestrator.execute_ecosystem_positioning_analysis(
                    region_info=lane_a_result.get("data"),
                    industry_info=lane_b_result.get("data")
                )
                
                step_result = {
                    "step": "stage2_dependency_analysis",
                    "status": "completed",
                    "ecosystem_positioning": ecosystem_analysis,
                    "dependencies_met": True,
                    "execution_time": (datetime.now() - stage_start).total_seconds(),
                    "timestamp": datetime.now().isoformat()
                }
            else:
                # ä¾èµ–æ¡ä»¶æœªæ»¡è¶³
                step_result = {
                    "step": "stage2_dependency_analysis",
                    "status": "skipped",
                    "ecosystem_positioning": None,
                    "dependencies_met": False,
                    "reason": "æ³³é“Aæˆ–æ³³é“BæœªæˆåŠŸå®Œæˆ",
                    "execution_time": (datetime.now() - stage_start).total_seconds(),
                    "timestamp": datetime.now().isoformat()
                }
            
            context["steps"].append(step_result)
            return step_result
            
        except Exception as e:
            step_result = {
                "step": "stage2_dependency_analysis",
                "status": "failed",
                "error": str(e),
                "execution_time": (datetime.now() - stage_start).total_seconds(),
                "timestamp": datetime.now().isoformat()
            }
            context["steps"].append(step_result)
            raise

    async def _execute_stage3_result_integration(self, stage1_result: Dict[str, Any],
                                               stage2_result: Dict[str, Any],
                                               context: Dict[str, Any]) -> Dict[str, Any]:
        """
        é˜¶æ®µä¸‰: ç»“æžœæ±‡æ€»
        """
        stage_start = datetime.now()
        
        try:
            # ç­‰å¾…æ‰€æœ‰æ³³é“å’Œé˜¶æ®µäºŒåˆ†æžå…¨éƒ¨å®Œæˆ
            all_results = {
                "lane_a": stage1_result.get("lane_a"),
                "lane_b": stage1_result.get("lane_b"),
                "lane_c": stage1_result.get("lane_c"),
                "lane_d": stage1_result.get("lane_d"),
                "lane_e": stage1_result.get("lane_e"),
                "lane_f": stage1_result.get("lane_f"),
                "ecosystem_positioning": stage2_result.get("ecosystem_positioning")
            }
            
            # æ•´åˆæ‰€æœ‰è¾“å‡ºï¼Œç”Ÿæˆæœ€ç»ˆç»“æž„åŒ–æŠ¥å‘Šï¼ˆåªå¯¹ç»“æž„è¿›è¡Œcheckï¼‰
            integrated_report = await self._generate_integrated_report(
                all_results, context
            )
            
            step_result = {
                "step": "stage3_result_integration",
                "status": "completed",
                "integrated_report": integrated_report,
                "data_completeness": self._calculate_completeness(all_results),
                "execution_time": (datetime.now() - stage_start).total_seconds(),
                "timestamp": datetime.now().isoformat()
            }
            
            context["steps"].append(step_result)
            return step_result
            
        except Exception as e:
            step_result = {
                "step": "stage3_result_integration",
                "status": "failed",
                "error": str(e),
                "execution_time": (datetime.now() - stage_start).total_seconds(),
                "timestamp": datetime.now().isoformat()
            }
            context["steps"].append(step_result)
            raise

    async def _generate_integrated_report(self, all_results: Dict[str, Any],
                                        context: Dict[str, Any]) -> Dict[str, Any]:
        """
        ç”Ÿæˆæ•´åˆæŠ¥å‘Šï¼ˆåªå¯¹ç»“æž„è¿›è¡Œcheckï¼Œä¸éœ€è¦codeï¼‰
        """
        try:
            # ç»“æž„æ£€æŸ¥å’ŒéªŒè¯
            report_structure = {
                "enterprise_info": {
                    "name": context["enterprise_name"],
                    "processing_timestamp": datetime.now().isoformat(),
                    "workflow_id": context["workflow_id"]
                },
                "data_sections": {},
                "structure_validation": {},
                "completeness_analysis": {},
                "quality_assessment": {}
            }
            
            # å¯¹æ¯ä¸ªæ•°æ®æºè¿›è¡Œç»“æž„æ£€æŸ¥
            for source_name, source_result in all_results.items():
                if source_result and source_result.get("status") == "completed":
                    # ç»“æž„éªŒè¯
                    structure_check = self._validate_data_structure(
                        source_name, source_result.get("data")
                    )
                    
                    report_structure["data_sections"][source_name] = {
                        "status": "available",
                        "data_type": self._identify_data_type(source_result.get("data")),
                        "structure_valid": structure_check["is_valid"],
                        "validation_details": structure_check
                    }
                    
                    report_structure["structure_validation"][source_name] = structure_check
                else:
                    report_structure["data_sections"][source_name] = {
                        "status": "unavailable",
                        "reason": source_result.get("error", "æœªçŸ¥é”™è¯¯") if source_result else "ç»“æžœä¸ºç©º"
                    }
                    
                    report_structure["structure_validation"][source_name] = {
                        "is_valid": False,
                        "issues": ["æ•°æ®ä¸å¯ç”¨"]
                    }
            
            # å®Œæ•´æ€§åˆ†æž
            report_structure["completeness_analysis"] = self._analyze_data_completeness(
                report_structure["data_sections"]
            )
            
            # è´¨é‡è¯„ä¼°
            report_structure["quality_assessment"] = self._assess_data_quality(
                report_structure["structure_validation"]
            )
            
            return report_structure
            
        except Exception as e:
            return {
                "error": f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}",
                "all_results": all_results,
                "context": context
            }

    def _validate_data_structure(self, source_name: str, data: Any) -> Dict[str, Any]:
        """
        éªŒè¯æ•°æ®ç»“æž„
        """
        validation_result = {
            "is_valid": False,
            "issues": [],
            "structure_info": {},
            "recommendations": []
        }
        
        try:
            if data is None:
                validation_result["issues"].append("æ•°æ®ä¸ºç©º")
                return validation_result
            
            if isinstance(data, dict):
                # æ£€æŸ¥å­—å…¸ç»“æž„
                validation_result["structure_info"] = {
                    "type": "object",
                    "keys": list(data.keys()),
                    "key_count": len(data.keys()),
                    "nested_objects": sum(1 for v in data.values() if isinstance(v, dict)),
                    "arrays": sum(1 for v in data.values() if isinstance(v, list))
                }
                
                # æ ¹æ®æ•°æ®æºæ£€æŸ¥å¿…éœ€å­—æ®µ
                required_fields = self._get_required_fields_by_source(source_name)
                missing_fields = [field for field in required_fields if field not in data]
                
                if missing_fields:
                    validation_result["issues"].append(f"ç¼ºå°‘å¿…éœ€å­—æ®µ: {', '.join(missing_fields)}")
                else:
                    validation_result["is_valid"] = True
                
                # æ£€æŸ¥æ•°æ®è´¨é‡
                if data.get("error"):
                    validation_result["issues"].append(f"æ•°æ®åŒ…å«é”™è¯¯ä¿¡æ¯: {data['error']}")
                
                if data.get("source") == "none":
                    validation_result["issues"].append("æ•°æ®æºæ ‡è®°ä¸ºæ— æ•ˆ")
                    validation_result["recommendations"].append("å»ºè®®æ£€æŸ¥æ•°æ®èŽ·å–é€»è¾‘")
                
            elif isinstance(data, list):
                # æ£€æŸ¥æ•°ç»„ç»“æž„
                validation_result["structure_info"] = {
                    "type": "array",
                    "length": len(data),
                    "item_types": list(set(type(item).__name__ for item in data)) if data else []
                }
                
                if len(data) > 0:
                    validation_result["is_valid"] = True
                else:
                    validation_result["issues"].append("æ•°ç»„ä¸ºç©º")
            
            else:
                # åŸºæœ¬æ•°æ®ç±»åž‹
                validation_result["structure_info"] = {
                    "type": type(data).__name__,
                    "value": str(data)[:100]  # é™åˆ¶é•¿åº¦
                }
                validation_result["is_valid"] = True
            
            return validation_result
            
        except Exception as e:
            validation_result["issues"].append(f"ç»“æž„éªŒè¯å¼‚å¸¸: {str(e)}")
            return validation_result

    def _identify_data_type(self, data: Any) -> str:
        """
        è¯†åˆ«æ•°æ®ç±»åž‹
        """
        if data is None:
            return "null"
        elif isinstance(data, dict):
            return "object"
        elif isinstance(data, list):
            return "array"
        elif isinstance(data, str):
            return "string"
        elif isinstance(data, (int, float)):
            return "number"
        elif isinstance(data, bool):
            return "boolean"
        else:
            return type(data).__name__

    def _analyze_data_completeness(self, data_sections: Dict[str, Any]) -> Dict[str, Any]:
        """
        åˆ†æžæ•°æ®å®Œæ•´æ€§
        """
        total_sections = len(data_sections)
        available_sections = sum(1 for section in data_sections.values() 
                               if section.get("status") == "available")
        
        completeness_score = available_sections / total_sections if total_sections > 0 else 0
        
        return {
            "total_sections": total_sections,
            "available_sections": available_sections,
            "unavailable_sections": total_sections - available_sections,
            "completeness_score": completeness_score,
            "completeness_percentage": f"{completeness_score * 100:.1f}%",
            "completeness_grade": self._grade_completeness(completeness_score)
        }

    def _assess_data_quality(self, structure_validation: Dict[str, Any]) -> Dict[str, Any]:
        """
        è¯„ä¼°æ•°æ®è´¨é‡
        """
        total_validations = len(structure_validation)
        valid_structures = sum(1 for validation in structure_validation.values()
                             if validation.get("is_valid", False))
        
        quality_score = valid_structures / total_validations if total_validations > 0 else 0
        
        # æ”¶é›†æ‰€æœ‰é—®é¢˜
        all_issues = []
        for source_name, validation in structure_validation.items():
            issues = validation.get("issues", [])
            for issue in issues:
                all_issues.append(f"{source_name}: {issue}")
        
        return {
            "total_validations": total_validations,
            "valid_structures": valid_structures,
            "invalid_structures": total_validations - valid_structures,
            "quality_score": quality_score,
            "quality_percentage": f"{quality_score * 100:.1f}%",
            "quality_grade": self._grade_quality(quality_score),
            "issues_summary": all_issues[:10],  # é™åˆ¶æ˜¾ç¤ºå‰10ä¸ªé—®é¢˜
            "total_issues": len(all_issues)
        }

    def _grade_completeness(self, score: float) -> str:
        """
        å®Œæ•´æ€§ç­‰çº§è¯„å®š
        """
        if score >= 0.9:
            return "ä¼˜ç§€"
        elif score >= 0.7:
            return "è‰¯å¥½"
        elif score >= 0.5:
            return "ä¸€èˆ¬"
        else:
            return "è¾ƒå·®"

    def _grade_quality(self, score: float) -> str:
        """
        è´¨é‡ç­‰çº§è¯„å®š
        """
        if score >= 0.95:
            return "ä¼˜ç§€"
        elif score >= 0.8:
            return "è‰¯å¥½"
        elif score >= 0.6:
            return "ä¸€èˆ¬"
        else:
            return "éœ€è¦æ”¹è¿›"

    def _calculate_completeness(self, all_results: Dict[str, Any]) -> Dict[str, Any]:
        """
        è®¡ç®—æ•´ä½“å®Œæ•´æ€§
        """
        total_lanes = len(all_results)
        completed_lanes = sum(1 for result in all_results.values()
                            if result and result.get("status") == "completed")
        
        return {
            "total_lanes": total_lanes,
            "completed_lanes": completed_lanes,
            "completion_rate": completed_lanes / total_lanes if total_lanes > 0 else 0,
            "missing_lanes": [name for name, result in all_results.items()
                            if not result or result.get("status") != "completed"]
        }

    def _get_required_fields_by_source(self, source_name: str) -> List[str]:
        """
        æ ¹æ®æ•°æ®æºèŽ·å–å¿…éœ€å­—æ®µ
        """
        field_mapping = {
            "lane_a": ["address_info", "region_info"],
            "lane_b": ["industry_info", "position_analysis"],
            "lane_c": ["personnel_scale", "scale_category"],
            "lane_d": ["revenue_info"],
            "lane_e": ["opportunities"],
            "lane_f": ["news_articles", "summary"],
            "ecosystem_positioning": ["ecosystem_analysis"]
        }
        
        return field_mapping.get(source_name, [])
```

## ðŸ”§ é…ç½®å’Œéƒ¨ç½²

### å·¥ä½œæµé…ç½®
```python
# config/workflow_config.py
from pydantic import BaseSettings
from typing import Dict, Any

class WorkflowConfig(BaseSettings):
    # å¹¶è¡Œå¤„ç†é…ç½®
    max_concurrent_lanes: int = 6
    lane_timeout_seconds: int = 30
    
    # æ•°æ®è¿‡æœŸé…ç½®
    address_expiry_years: int = 1
    industry_expiry_years: int = 1
    scale_expiry_months: int = 6
    revenue_expiry_months: int = 6
    
    # é‡è¯•é…ç½®
    max_retries: int = 3
    retry_delay_seconds: int = 2
    
    # ç¼“å­˜é…ç½®
    enable_result_caching: bool = True
    cache_expiry_hours: int = 24
    
    # ç›‘æŽ§é…ç½®
    enable_performance_monitoring: bool = True
    log_detailed_execution: bool = True
    
    class Config:
        env_prefix = "WORKFLOW_"
```

### å·¥ä½œæµç›‘æŽ§
```python
# monitoring/workflow_monitor.py
import asyncio
from typing import Dict, Any, List
from datetime import datetime, timedelta
from dataclasses import dataclass

@dataclass
class WorkflowMetrics:
    workflow_id: str
    enterprise_name: str
    start_time: datetime
    end_time: datetime = None
    status: str = "running"
    stage1_duration: float = 0.0
    stage2_duration: float = 0.0
    stage3_duration: float = 0.0
    total_duration: float = 0.0
    lane_metrics: Dict[str, Any] = None
    error_count: int = 0
    success_rate: float = 0.0

class WorkflowMonitor:
    def __init__(self):
        self.active_workflows: Dict[str, WorkflowMetrics] = {}
        self.completed_workflows: List[WorkflowMetrics] = []
    
    async def start_monitoring(self, workflow_id: str, enterprise_name: str):
        """
        å¼€å§‹ç›‘æŽ§å·¥ä½œæµ
        """
        metrics = WorkflowMetrics(
            workflow_id=workflow_id,
            enterprise_name=enterprise_name,
            start_time=datetime.now()
        )
        
        self.active_workflows[workflow_id] = metrics
        return metrics
    
    async def update_stage_metrics(self, workflow_id: str, stage: str, duration: float):
        """
        æ›´æ–°é˜¶æ®µæŒ‡æ ‡
        """
        if workflow_id in self.active_workflows:
            metrics = self.active_workflows[workflow_id]
            
            if stage == "stage1":
                metrics.stage1_duration = duration
            elif stage == "stage2":
                metrics.stage2_duration = duration
            elif stage == "stage3":
                metrics.stage3_duration = duration
    
    async def complete_monitoring(self, workflow_id: str, success: bool):
        """
        å®Œæˆå·¥ä½œæµç›‘æŽ§
        """
        if workflow_id in self.active_workflows:
            metrics = self.active_workflows[workflow_id]
            metrics.end_time = datetime.now()
            metrics.total_duration = (metrics.end_time - metrics.start_time).total_seconds()
            metrics.status = "completed" if success else "failed"
            
            # ç§»åŠ¨åˆ°å·²å®Œæˆåˆ—è¡¨
            self.completed_workflows.append(metrics)
            del self.active_workflows[workflow_id]
    
    def get_performance_summary(self) -> Dict[str, Any]:
        """
        èŽ·å–æ€§èƒ½æ‘˜è¦
        """
        if not self.completed_workflows:
            return {"status": "no_data"}
        
        total_workflows = len(self.completed_workflows)
        successful_workflows = sum(1 for w in self.completed_workflows if w.status == "completed")
        
        avg_duration = sum(w.total_duration for w in self.completed_workflows) / total_workflows
        
        return {
            "total_workflows": total_workflows,
            "successful_workflows": successful_workflows,
            "success_rate": successful_workflows / total_workflows,
            "average_duration": avg_duration,
            "active_workflows": len(self.active_workflows),
            "performance_grade": self._grade_performance(successful_workflows / total_workflows, avg_duration)
        }
    
    def _grade_performance(self, success_rate: float, avg_duration: float) -> str:
        """
        æ€§èƒ½ç­‰çº§è¯„å®š
        """
        if success_rate >= 0.95 and avg_duration <= 30:
            return "ä¼˜ç§€"
        elif success_rate >= 0.90 and avg_duration <= 60:
            return "è‰¯å¥½"
        elif success_rate >= 0.80 and avg_duration <= 120:
            return "ä¸€èˆ¬"
        else:
            return "éœ€è¦ä¼˜åŒ–"
```

## ðŸ“Š ä½¿ç”¨ç¤ºä¾‹

### APIè°ƒç”¨ç¤ºä¾‹
```bash
# å¤„ç†ä¼ä¸šä¿¡æ¯å·¥ä½œæµ
curl -X POST "http://localhost:9003/api/v1/workflow/process" \
  -H "Content-Type: application/json" \
  -d '{
    "enterprise_name": "é’å²›æµ·å°”æ™ºå®¶è‚¡ä»½æœ‰é™å…¬å¸",
    "force_refresh": false
  }'

# èŽ·å–å·¥ä½œæµçŠ¶æ€
curl -X GET "http://localhost:9003/api/v1/workflow/status/{workflow_id}"

# èŽ·å–æ€§èƒ½ç›‘æŽ§æ•°æ®
curl -X GET "http://localhost:9003/api/v1/workflow/metrics"
```

### å“åº”ç¤ºä¾‹
```json
{
  "workflow_id": "wf_20250926_001",
  "status": "success",
  "result": {
    "workflow_type": "existing_enterprise_processing",
    "enterprise_id": 123,
    "enterprise_name": "é’å²›æµ·å°”æ™ºå®¶è‚¡ä»½æœ‰é™å…¬å¸",
    "stage1_result": {
      "lane_a": {
        "status": "completed",
        "data": {
          "source": "local",
          "address_info": {...},
          "region_info": {...}
        }
      },
      "lane_b": {...},
      "lane_c": {...},
      "lane_d": {...},
      "lane_e": {...},
      "lane_f": {...}
    },
    "stage2_result": {
      "status": "completed",
      "ecosystem_positioning": {...}
    },
    "final_result": {
      "integrated_report": {...},
      "data_completeness": {
        "completion_rate": 0.85,
        "completeness_grade": "è‰¯å¥½"
      }
    }
  },
  "timestamp": "2025-09-26T11:30:00Z"
}
```

---

*æ–‡æ¡£ç‰ˆæœ¬ï¼šv1.0*
*æ›´æ–°æ—¶é—´ï¼š2025å¹´9æœˆ26æ—¥*