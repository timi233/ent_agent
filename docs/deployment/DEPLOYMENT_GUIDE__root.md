# éƒ¨ç½²æŒ‡å—

## ğŸ“‹ æ¦‚è¿°

æœ¬æ–‡æ¡£è¯¦ç»†æè¿°äº†åŸå¸‚å¤§è„‘ä¼ä¸šä¿¡æ¯å¤„ç†ç³»ç»Ÿçš„éƒ¨ç½²æ–¹æ¡ˆï¼ŒåŒ…æ‹¬å¼€å‘ç¯å¢ƒã€æµ‹è¯•ç¯å¢ƒå’Œç”Ÿäº§ç¯å¢ƒçš„éƒ¨ç½²é…ç½®ã€‚

## ğŸ—ï¸ ç³»ç»Ÿæ¶æ„éƒ¨ç½²å›¾

```mermaid
graph TB
    subgraph "è´Ÿè½½å‡è¡¡å±‚"
        LB[Nginxè´Ÿè½½å‡è¡¡å™¨]
    end
    
    subgraph "åº”ç”¨å±‚"
        FE1[å‰ç«¯æœåŠ¡1]
        FE2[å‰ç«¯æœåŠ¡2]
        BE1[åç«¯æœåŠ¡1]
        BE2[åç«¯æœåŠ¡2]
    end
    
    subgraph "ç¼“å­˜å±‚"
        REDIS[Redisé›†ç¾¤]
    end
    
    subgraph "æ•°æ®å±‚"
        DB[MySQLä¸»ä»é›†ç¾¤]
    end
    
    subgraph "å¤–éƒ¨æœåŠ¡"
        BOCHAAI[åšæŸ¥AI API]
        DEEPSEEK[DeepSeek API]
    end
    
    LB --> FE1
    LB --> FE2
    LB --> BE1
    LB --> BE2
    
    BE1 --> REDIS
    BE2 --> REDIS
    BE1 --> DB
    BE2 --> DB
    
    BE1 --> BOCHAAI
    BE1 --> DEEPSEEK
    BE2 --> BOCHAAI
    BE2 --> DEEPSEEK
```

## ğŸ³ Dockerå®¹å™¨åŒ–éƒ¨ç½²

### åç«¯Dockerfile
```dockerfile
# Dockerfile.backend
FROM python:3.11-slim

WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    && rm -rf /var/lib/apt/lists/*

# å¤åˆ¶ä¾èµ–æ–‡ä»¶
COPY requirements.txt .

# å®‰è£…Pythonä¾èµ–
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY . .

# åˆ›å»ºæ—¥å¿—ç›®å½•
RUN mkdir -p /app/logs

# è®¾ç½®ç¯å¢ƒå˜é‡
ENV PYTHONPATH=/app
ENV PYTHONUNBUFFERED=1

# æš´éœ²ç«¯å£
EXPOSE 9003

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:9003/health || exit 1

# å¯åŠ¨å‘½ä»¤
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "9003", "--workers", "4"]
```

### å‰ç«¯Dockerfile
```dockerfile
# Dockerfile.frontend
FROM node:18-alpine AS builder

WORKDIR /app

# å¤åˆ¶packageæ–‡ä»¶
COPY package*.json ./

# å®‰è£…ä¾èµ–
RUN npm ci --only=production

# å¤åˆ¶æºä»£ç 
COPY . .

# æ„å»ºåº”ç”¨
RUN npm run build

# ç”Ÿäº§ç¯å¢ƒé•œåƒ
FROM nginx:alpine

# å¤åˆ¶æ„å»ºç»“æœ
COPY --from=builder /app/dist /usr/share/nginx/html

# å¤åˆ¶nginxé…ç½®
COPY nginx.conf /etc/nginx/nginx.conf

# æš´éœ²ç«¯å£
EXPOSE 80

# å¯åŠ¨nginx
CMD ["nginx", "-g", "daemon off;"]
```

### Docker Composeå®Œæ•´é…ç½®
```yaml
# docker-compose.prod.yml
version: '3.8'

services:
  # Nginxè´Ÿè½½å‡è¡¡å™¨
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf
      - ./nginx/ssl:/etc/nginx/ssl
      - ./logs/nginx:/var/log/nginx
    depends_on:
      - frontend1
      - frontend2
      - backend1
      - backend2
    restart: unless-stopped
    networks:
      - city_brain_network

  # å‰ç«¯æœåŠ¡
  frontend1:
    build:
      context: ./city_brain_frontend
      dockerfile: Dockerfile
    restart: unless-stopped
    networks:
      - city_brain_network

  frontend2:
    build:
      context: ./city_brain_frontend
      dockerfile: Dockerfile
    restart: unless-stopped
    networks:
      - city_brain_network

  # åç«¯æœåŠ¡
  backend1:
    build:
      context: ./city_brain_system
      dockerfile: Dockerfile
    environment:
      - DATABASE_URL=mysql+asyncio://city_brain_user:${DB_PASSWORD}@mysql-master:3306/city_brain_db
      - REDIS_HOST=redis-master
      - REDIS_PORT=6379
      - BOCHAAI_API_KEY=${BOCHAAI_API_KEY}
      - DEEPSEEK_API_KEY=${DEEPSEEK_API_KEY}
      - LOG_LEVEL=INFO
    volumes:
      - ./logs/backend:/app/logs
    depends_on:
      - mysql-master
      - redis-master
    restart: unless-stopped
    networks:
      - city_brain_network

  backend2:
    build:
      context: ./city_brain_system
      dockerfile: Dockerfile
    environment:
      - DATABASE_URL=mysql+asyncio://city_brain_user:${DB_PASSWORD}@mysql-master:3306/city_brain_db
      - REDIS_HOST=redis-master
      - REDIS_PORT=6379
      - BOCHAAI_API_KEY=${BOCHAAI_API_KEY}
      - DEEPSEEK_API_KEY=${DEEPSEEK_API_KEY}
      - LOG_LEVEL=INFO
    volumes:
      - ./logs/backend:/app/logs
    depends_on:
      - mysql-master
      - redis-master
    restart: unless-stopped
    networks:
      - city_brain_network

  # MySQLä¸»ä»é›†ç¾¤
  mysql-master:
    image: mysql:8.0
    environment:
      MYSQL_ROOT_PASSWORD: ${DB_ROOT_PASSWORD}
      MYSQL_DATABASE: city_brain_db
      MYSQL_USER: city_brain_user
      MYSQL_PASSWORD: ${DB_PASSWORD}
      MYSQL_REPLICATION_USER: replication_user
      MYSQL_REPLICATION_PASSWORD: ${REPLICATION_PASSWORD}
    command: >
      --server-id=1
      --log-bin=mysql-bin
      --binlog-format=ROW
      --gtid-mode=ON
      --enforce-gtid-consistency=ON
      --log-slave-updates=ON
      --binlog-do-db=city_brain_db
    volumes:
      - mysql_master_data:/var/lib/mysql
      - ./mysql/master.cnf:/etc/mysql/conf.d/master.cnf
      - ./logs/mysql:/var/log/mysql
    ports:
      - "3306:3306"
    restart: unless-stopped
    networks:
      - city_brain_network

  mysql-slave:
    image: mysql:8.0
    environment:
      MYSQL_ROOT_PASSWORD: ${DB_ROOT_PASSWORD}
      MYSQL_DATABASE: city_brain_db
      MYSQL_USER: city_brain_user
      MYSQL_PASSWORD: ${DB_PASSWORD}
    command: >
      --server-id=2
      --relay-log=mysql-relay-bin
      --log-bin=mysql-bin
      --binlog-format=ROW
      --gtid-mode=ON
      --enforce-gtid-consistency=ON
      --log-slave-updates=ON
      --read-only=ON
      --skip-slave-start
    volumes:
      - mysql_slave_data:/var/lib/mysql
      - ./mysql/slave.cnf:/etc/mysql/conf.d/slave.cnf
    depends_on:
      - mysql-master
    restart: unless-stopped
    networks:
      - city_brain_network

  # Redisä¸»ä»é›†ç¾¤
  redis-master:
    image: redis:7-alpine
    command: >
      redis-server
      --appendonly yes
      --appendfsync everysec
      --save 900 1
      --save 300 10
      --save 60 10000
      --maxmemory 2gb
      --maxmemory-policy allkeys-lru
    volumes:
      - redis_master_data:/data
      - ./logs/redis:/var/log/redis
    ports:
      - "6379:6379"
    restart: unless-stopped
    networks:
      - city_brain_network

  redis-slave:
    image: redis:7-alpine
    command: >
      redis-server
      --slaveof redis-master 6379
      --appendonly yes
      --appendfsync everysec
      --save 900 1
      --save 300 10
      --save 60 10000
      --maxmemory 1gb
      --maxmemory-policy allkeys-lru
    volumes:
      - redis_slave_data:/data
    depends_on:
      - redis-master
    restart: unless-stopped
    networks:
      - city_brain_network

  # ç›‘æ§æœåŠ¡
  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=200h'
      - '--web.enable-lifecycle'
    restart: unless-stopped
    networks:
      - city_brain_network

  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD}
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards
      - ./monitoring/grafana/datasources:/etc/grafana/provisioning/datasources
    restart: unless-stopped
    networks:
      - city_brain_network

volumes:
  mysql_master_data:
  mysql_slave_data:
  redis_master_data:
  redis_slave_data:
  prometheus_data:
  grafana_data:

networks:
  city_brain_network:
    driver: bridge
```

## ğŸ”§ Nginxé…ç½®

### è´Ÿè½½å‡è¡¡é…ç½®
```nginx
# nginx/nginx.conf
user nginx;
worker_processes auto;
error_log /var/log/nginx/error.log warn;
pid /var/run/nginx.pid;

events {
    worker_connections 1024;
    use epoll;
    multi_accept on;
}

http {
    include /etc/nginx/mime.types;
    default_type application/octet-stream;

    # æ—¥å¿—æ ¼å¼
    log_format main '$remote_addr - $remote_user [$time_local] "$request" '
                    '$status $body_bytes_sent "$http_referer" '
                    '"$http_user_agent" "$http_x_forwarded_for" '
                    'rt=$request_time uct="$upstream_connect_time" '
                    'uht="$upstream_header_time" urt="$upstream_response_time"';

    access_log /var/log/nginx/access.log main;

    # åŸºç¡€é…ç½®
    sendfile on;
    tcp_nopush on;
    tcp_nodelay on;
    keepalive_timeout 65;
    types_hash_max_size 2048;
    client_max_body_size 100M;

    # Gzipå‹ç¼©
    gzip on;
    gzip_vary on;
    gzip_min_length 1024;
    gzip_proxied any;
    gzip_comp_level 6;
    gzip_types
        text/plain
        text/css
        text/xml
        text/javascript
        application/json
        application/javascript
        application/xml+rss
        application/atom+xml
        image/svg+xml;

    # ä¸Šæ¸¸æœåŠ¡å™¨é…ç½®
    upstream frontend_servers {
        least_conn;
        server frontend1:80 max_fails=3 fail_timeout=30s;
        server frontend2:80 max_fails=3 fail_timeout=30s;
    }

    upstream backend_servers {
        least_conn;
        server backend1:9003 max_fails=3 fail_timeout=30s;
        server backend2:9003 max_fails=3 fail_timeout=30s;
    }

    # é™æµé…ç½®
    limit_req_zone $binary_remote_addr zone=api:10m rate=10r/s;
    limit_req_zone $binary_remote_addr zone=login:10m rate=1r/s;

    # ä¸»æœåŠ¡å™¨é…ç½®
    server {
        listen 80;
        server_name your-domain.com;

        # é‡å®šå‘åˆ°HTTPS
        return 301 https://$server_name$request_uri;
    }

    server {
        listen 443 ssl http2;
        server_name your-domain.com;

        # SSLé…ç½®
        ssl_certificate /etc/nginx/ssl/cert.pem;
        ssl_certificate_key /etc/nginx/ssl/key.pem;
        ssl_session_timeout 1d;
        ssl_session_cache shared:SSL:50m;
        ssl_session_tickets off;

        # ç°ä»£SSLé…ç½®
        ssl_protocols TLSv1.2 TLSv1.3;
        ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE-RSA-AES256-GCM-SHA384;
        ssl_prefer_server_ciphers off;

        # HSTS
        add_header Strict-Transport-Security "max-age=63072000" always;

        # å‰ç«¯é™æ€èµ„æº
        location / {
            proxy_pass http://frontend_servers;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            
            # ç¼“å­˜é…ç½®
            location ~* \.(js|css|png|jpg|jpeg|gif|ico|svg|woff|woff2|ttf|eot)$ {
                expires 1y;
                add_header Cache-Control "public, immutable";
            }
        }

        # APIæ¥å£
        location /api/ {
            limit_req zone=api burst=20 nodelay;
            
            proxy_pass http://backend_servers;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            
            # è¶…æ—¶é…ç½®
            proxy_connect_timeout 30s;
            proxy_send_timeout 60s;
            proxy_read_timeout 60s;
            
            # ç¼“å†²é…ç½®
            proxy_buffering on;
            proxy_buffer_size 4k;
            proxy_buffers 8 4k;
        }

        # ç™»å½•æ¥å£ç‰¹æ®Šé™æµ
        location /api/auth/login {
            limit_req zone=login burst=5 nodelay;
            
            proxy_pass http://backend_servers;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
        }

        # å¥åº·æ£€æŸ¥
        location /health {
            access_log off;
            return 200 "healthy\n";
            add_header Content-Type text/plain;
        }

        # ç›‘æ§ç«¯ç‚¹
        location /metrics {
            allow 127.0.0.1;
            allow 10.0.0.0/8;
            deny all;
            
            proxy_pass http://backend_servers/metrics;
        }
    }
}
```

## ğŸš€ éƒ¨ç½²è„šæœ¬

### è‡ªåŠ¨åŒ–éƒ¨ç½²è„šæœ¬
```bash
#!/bin/bash
# deploy.sh

set -e

# é…ç½®å˜é‡
PROJECT_NAME="city-brain"
DOCKER_REGISTRY="your-registry.com"
VERSION=${1:-latest}
ENVIRONMENT=${2:-production}

echo "ğŸš€ å¼€å§‹éƒ¨ç½²åŸå¸‚å¤§è„‘ç³»ç»Ÿ - ç‰ˆæœ¬: $VERSION, ç¯å¢ƒ: $ENVIRONMENT"

# æ£€æŸ¥å¿…è¦çš„ç¯å¢ƒå˜é‡
check_env_vars() {
    local required_vars=(
        "DB_PASSWORD"
        "DB_ROOT_PASSWORD"
        "REPLICATION_PASSWORD"
        "BOCHAAI_API_KEY"
        "DEEPSEEK_API_KEY"
        "GRAFANA_PASSWORD"
    )
    
    for var in "${required_vars[@]}"; do
        if [[ -z "${!var}" ]]; then
            echo "âŒ ç¯å¢ƒå˜é‡ $var æœªè®¾ç½®"
            exit 1
        fi
    done
    
    echo "âœ… ç¯å¢ƒå˜é‡æ£€æŸ¥é€šè¿‡"
}

# æ„å»ºé•œåƒ
build_images() {
    echo "ğŸ”¨ æ„å»ºDockeré•œåƒ..."
    
    # æ„å»ºåç«¯é•œåƒ
    docker build -t $DOCKER_REGISTRY/$PROJECT_NAME-backend:$VERSION ./city_brain_system
    
    # æ„å»ºå‰ç«¯é•œåƒ
    docker build -t $DOCKER_REGISTRY/$PROJECT_NAME-frontend:$VERSION ./city_brain_frontend
    
    echo "âœ… é•œåƒæ„å»ºå®Œæˆ"
}

# æ¨é€é•œåƒåˆ°ä»“åº“
push_images() {
    echo "ğŸ“¤ æ¨é€é•œåƒåˆ°ä»“åº“..."
    
    docker push $DOCKER_REGISTRY/$PROJECT_NAME-backend:$VERSION
    docker push $DOCKER_REGISTRY/$PROJECT_NAME-frontend:$VERSION
    
    echo "âœ… é•œåƒæ¨é€å®Œæˆ"
}

# éƒ¨ç½²æœåŠ¡
deploy_services() {
    echo "ğŸš€ éƒ¨ç½²æœåŠ¡..."
    
    # åˆ›å»ºå¿…è¦çš„ç›®å½•
    mkdir -p logs/{nginx,backend,mysql,redis}
    mkdir -p mysql nginx/ssl monitoring/{prometheus,grafana}
    
    # åœæ­¢ç°æœ‰æœåŠ¡
    docker-compose -f docker-compose.prod.yml down
    
    # å¯åŠ¨æ–°æœåŠ¡
    docker-compose -f docker-compose.prod.yml up -d
    
    echo "âœ… æœåŠ¡éƒ¨ç½²å®Œæˆ"
}

# ç­‰å¾…æœåŠ¡å¯åŠ¨
wait_for_services() {
    echo "â³ ç­‰å¾…æœåŠ¡å¯åŠ¨..."
    
    local max_attempts=30
    local attempt=1
    
    while [[ $attempt -le $max_attempts ]]; do
        if curl -f http://localhost/health > /dev/null 2>&1; then
            echo "âœ… æœåŠ¡å¯åŠ¨æˆåŠŸ"
            return 0
        fi
        
        echo "å°è¯• $attempt/$max_attempts - ç­‰å¾…æœåŠ¡å¯åŠ¨..."
        sleep 10
        ((attempt++))
    done
    
    echo "âŒ æœåŠ¡å¯åŠ¨è¶…æ—¶"
    exit 1
}

# è¿è¡Œå¥åº·æ£€æŸ¥
health_check() {
    echo "ğŸ” è¿è¡Œå¥åº·æ£€æŸ¥..."
    
    # æ£€æŸ¥å‰ç«¯
    if curl -f http://localhost/ > /dev/null 2>&1; then
        echo "âœ… å‰ç«¯æœåŠ¡æ­£å¸¸"
    else
        echo "âŒ å‰ç«¯æœåŠ¡å¼‚å¸¸"
        exit 1
    fi
    
    # æ£€æŸ¥åç«¯API
    if curl -f http://localhost/api/health > /dev/null 2>&1; then
        echo "âœ… åç«¯APIæ­£å¸¸"
    else
        echo "âŒ åç«¯APIå¼‚å¸¸"
        exit 1
    fi
    
    # æ£€æŸ¥æ•°æ®åº“è¿æ¥
    if docker-compose -f docker-compose.prod.yml exec -T backend1 python -c "
import asyncio
from database.connection import get_database
async def test():
    db = await get_database()
    await db.execute('SELECT 1')
    print('Database OK')
asyncio.run(test())
" > /dev/null 2>&1; then
        echo "âœ… æ•°æ®åº“è¿æ¥æ­£å¸¸"
    else
        echo "âŒ æ•°æ®åº“è¿æ¥å¼‚å¸¸"
        exit 1
    fi
    
    echo "âœ… å¥åº·æ£€æŸ¥é€šè¿‡"
}

# æ¸…ç†æ—§é•œåƒ
cleanup() {
    echo "ğŸ§¹ æ¸…ç†æ—§é•œåƒ..."
    
    # åˆ é™¤æœªä½¿ç”¨çš„é•œåƒ
    docker image prune -f
    
    # åˆ é™¤æœªä½¿ç”¨çš„å®¹å™¨
    docker container prune -f
    
    echo "âœ… æ¸…ç†å®Œæˆ"
}

# ä¸»æ‰§è¡Œæµç¨‹
main() {
    check_env_vars
    build_images
    
    if [[ "$ENVIRONMENT" == "production" ]]; then
        push_images
    fi
    
    deploy_services
    wait_for_services
    health_check
    cleanup
    
    echo "ğŸ‰ éƒ¨ç½²å®Œæˆï¼"
    echo "å‰ç«¯åœ°å€: https://your-domain.com"
    echo "ç›‘æ§åœ°å€: http://your-domain.com:3000"
    echo "APIæ–‡æ¡£: https://your-domain.com/api/docs"
}

# æ‰§è¡Œä¸»æµç¨‹
main "$@"
```

### æ•°æ®åº“åˆå§‹åŒ–è„šæœ¬
```bash
#!/bin/bash
# init-database.sh

set -e

echo "ğŸ—„ï¸ åˆå§‹åŒ–æ•°æ®åº“..."

# ç­‰å¾…MySQLå¯åŠ¨
wait_for_mysql() {
    local max_attempts=30
    local attempt=1
    
    while [[ $attempt -le $max_attempts ]]; do
        if docker-compose -f docker-compose.prod.yml exec -T mysql-master mysql -uroot -p$DB_ROOT_PASSWORD -e "SELECT 1" > /dev/null 2>&1; then
            echo "âœ… MySQLå·²å¯åŠ¨"
            return 0
        fi
        
        echo "å°è¯• $attempt/$max_attempts - ç­‰å¾…MySQLå¯åŠ¨..."
        sleep 5
        ((attempt++))
    done
    
    echo "âŒ MySQLå¯åŠ¨è¶…æ—¶"
    exit 1
}

# åˆ›å»ºæ•°æ®åº“è¡¨
create_tables() {
    echo "ğŸ“‹ åˆ›å»ºæ•°æ®åº“è¡¨..."
    
    docker-compose -f docker-compose.prod.yml exec -T mysql-master mysql -uroot -p$DB_ROOT_PASSWORD city_brain_db < database/schema.sql
    
    echo "âœ… æ•°æ®åº“è¡¨åˆ›å»ºå®Œæˆ"
}

# å¯¼å…¥åˆå§‹æ•°æ®
import_initial_data() {
    echo "ğŸ“¥ å¯¼å…¥åˆå§‹æ•°æ®..."
    
    if [[ -f "database/initial_data.sql" ]]; then
        docker-compose -f docker-compose.prod.yml exec -T mysql-master mysql -uroot -p$DB_ROOT_PASSWORD city_brain_db < database/initial_data.sql
        echo "âœ… åˆå§‹æ•°æ®å¯¼å…¥å®Œæˆ"
    else
        echo "âš ï¸ æœªæ‰¾åˆ°åˆå§‹æ•°æ®æ–‡ä»¶ï¼Œè·³è¿‡å¯¼å…¥"
    fi
}

# é…ç½®ä¸»ä»å¤åˆ¶
setup_replication() {
    echo "ğŸ”„ é…ç½®MySQLä¸»ä»å¤åˆ¶..."
    
    # åœ¨ä¸»æœåŠ¡å™¨ä¸Šåˆ›å»ºå¤åˆ¶ç”¨æˆ·
    docker-compose -f docker-compose.prod.yml exec -T mysql-master mysql -uroot -p$DB_ROOT_PASSWORD -e "
        CREATE USER IF NOT EXISTS 'replication_user'@'%' IDENTIFIED BY '$REPLICATION_PASSWORD';
        GRANT REPLICATION SLAVE ON *.* TO 'replication_user'@'%';
        FLUSH PRIVILEGES;
    "
    
    # è·å–ä¸»æœåŠ¡å™¨çŠ¶æ€
    MASTER_STATUS=$(docker-compose -f docker-compose.prod.yml exec -T mysql-master mysql -uroot -p$DB_ROOT_PASSWORD -e "SHOW MASTER STATUS\G")
    MASTER_LOG_FILE=$(echo "$MASTER_STATUS" | grep "File:" | awk '{print $2}')
    MASTER_LOG_POS=$(echo "$MASTER_STATUS" | grep "Position:" | awk '{print $2}')
    
    # é…ç½®ä»æœåŠ¡å™¨
    docker-compose -f docker-compose.prod.yml exec -T mysql-slave mysql -uroot -p$DB_ROOT_PASSWORD -e "
        CHANGE MASTER TO
        MASTER_HOST='mysql-master',
        MASTER_USER='replication_user',
        MASTER_PASSWORD='$REPLICATION_PASSWORD',
        MASTER_LOG_FILE='$MASTER_LOG_FILE',
        MASTER_LOG_POS=$MASTER_LOG_POS;
        START SLAVE;
    "
    
    echo "âœ… MySQLä¸»ä»å¤åˆ¶é…ç½®å®Œæˆ"
}

# ä¸»æ‰§è¡Œæµç¨‹
main() {
    wait_for_mysql
    create_tables
    import_initial_data
    setup_replication
    
    echo "ğŸ‰ æ•°æ®åº“åˆå§‹åŒ–å®Œæˆï¼"
}

main "$@"
```

## ğŸ“Š ç›‘æ§é…ç½®

### Prometheusé…ç½®
```yaml
# monitoring/prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "rules/*.yml"

scrape_configs:
  - job_name: 'city-brain-backend'
    static_configs:
      - targets: ['backend1:9003', 'backend2:9003']
    metrics_path: '/metrics'
    scrape_interval: 30s

  - job_name: 'nginx'
    static_configs:
      - targets: ['nginx:80']
    metrics_path: '/metrics'

  - job_name: 'mysql'
    static_configs:
      - targets: ['mysql-master:3306', 'mysql-slave:3306']

  - job_name: 'redis'
    static_configs:
      - targets: ['redis-master:6379', 'redis-slave:6379']

alerting:
  alertmanagers:
    - static_configs:
        - targets:
          - alertmanager:9093
```

---

*æ–‡æ¡£ç‰ˆæœ¬ï¼šv1.0*
*æ›´æ–°æ—¶é—´ï¼š2025å¹´9æœˆ26æ—¥*