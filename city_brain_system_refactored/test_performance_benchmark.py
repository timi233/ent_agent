"""
Phase 6.2: æ€§èƒ½åŸºå‡†æµ‹è¯•
è¯„ä¼°ç³»ç»Ÿå“åº”æ—¶é—´ã€å¹¶å‘èƒ½åŠ›å’Œèµ„æºä½¿ç”¨æƒ…å†µ
"""

import os
import sys
import time
import asyncio
import threading
import json
import statistics
from datetime import datetime
from typing import Dict, Any, List
from concurrent.futures import ThreadPoolExecutor, as_completed
import psutil

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(__file__))

from config.simple_settings import load_settings
from infrastructure.database.connection import get_database_connection
from infrastructure.database.standalone_queries import (
    get_customers, get_enterprises, get_industries,
    search_customers
)
from domain.services.enterprise_service import EnterpriseService
from api.v1.dependencies import ServiceContainer


class PerformanceBenchmark:
    """æ€§èƒ½åŸºå‡†æµ‹è¯•å™¨"""
    
    def __init__(self):
        self.settings = load_settings()
        self.results = {}
        self.start_time = time.time()
        
    def log_result(self, test_name: str, metrics: Dict[str, Any]):
        """è®°å½•æµ‹è¯•ç»“æœ"""
        self.results[test_name] = {
            **metrics,
            "timestamp": datetime.now().isoformat()
        }
        print(f"ğŸ“Š {test_name}: {metrics}")
    
    def measure_response_time(self, func, *args, **kwargs):
        """æµ‹é‡å‡½æ•°å“åº”æ—¶é—´"""
        start = time.time()
        try:
            result = func(*args, **kwargs)
            duration = time.time() - start
            return {"success": True, "duration": duration, "result": result}
        except Exception as e:
            duration = time.time() - start
            return {"success": False, "duration": duration, "error": str(e)}
    
    def test_database_performance(self):
        """æµ‹è¯•æ•°æ®åº“æ€§èƒ½"""
        print("ğŸ” æµ‹è¯•æ•°æ®åº“æ€§èƒ½...")
        
        # æµ‹è¯•å•æ¬¡æŸ¥è¯¢æ€§èƒ½
        single_query_times = []
        for i in range(10):
            result = self.measure_response_time(get_all_customers)
            if result["success"]:
                single_query_times.append(result["duration"])
        
        # æµ‹è¯•æ‰¹é‡æŸ¥è¯¢æ€§èƒ½
        batch_query_times = []
        for i in range(5):
            start = time.time()
            try:
                customers = get_all_customers()
                enterprises = get_all_enterprises()
                industries = get_all_industries()
                duration = time.time() - start
                batch_query_times.append(duration)
            except Exception as e:
                print(f"æ‰¹é‡æŸ¥è¯¢å¤±è´¥: {e}")
        
        # æµ‹è¯•æœç´¢æ€§èƒ½
        search_times = []
        test_keywords = ["æµ·å°”", "åä¸º", "è…¾è®¯", "é˜¿é‡Œ", "ç™¾åº¦"]
        for keyword in test_keywords:
            result = self.measure_response_time(search_customers_by_name, keyword)
            if result["success"]:
                search_times.append(result["duration"])
        
        metrics = {
            "single_query": {
                "avg_time": statistics.mean(single_query_times) if single_query_times else 0,
                "min_time": min(single_query_times) if single_query_times else 0,
                "max_time": max(single_query_times) if single_query_times else 0,
                "success_rate": len(single_query_times) / 10 * 100
            },
            "batch_query": {
                "avg_time": statistics.mean(batch_query_times) if batch_query_times else 0,
                "min_time": min(batch_query_times) if batch_query_times else 0,
                "max_time": max(batch_query_times) if batch_query_times else 0,
                "success_rate": len(batch_query_times) / 5 * 100
            },
            "search_query": {
                "avg_time": statistics.mean(search_times) if search_times else 0,
                "min_time": min(search_times) if search_times else 0,
                "max_time": max(search_times) if search_times else 0,
                "success_rate": len(search_times) / len(test_keywords) * 100
            }
        }
        
        self.log_result("æ•°æ®åº“æ€§èƒ½", metrics)
        return metrics
    
    def test_business_service_performance(self):
        """æµ‹è¯•ä¸šåŠ¡æœåŠ¡æ€§èƒ½"""
        print("ğŸ§  æµ‹è¯•ä¸šåŠ¡æœåŠ¡æ€§èƒ½...")
        
        enterprise_service = EnterpriseService()
        
        # æµ‹è¯•ä¼ä¸šåç§°æå–æ€§èƒ½
        extraction_times = []
        test_inputs = [
            "æŸ¥è¯¢æµ·å°”é›†å›¢çš„ä¿¡æ¯",
            "åä¸ºæŠ€æœ¯æœ‰é™å…¬å¸æ€ä¹ˆæ ·",
            "è…¾è®¯æ§è‚¡çš„è¯¦ç»†èµ„æ–™",
            "é˜¿é‡Œå·´å·´é›†å›¢çš„ä¸šåŠ¡èŒƒå›´",
            "ç™¾åº¦å…¬å¸çš„å‘å±•å†ç¨‹"
        ]
        
        for input_text in test_inputs:
            result = self.measure_response_time(
                enterprise_service.search_service.extract_company_name_from_input,
                input_text
            )
            if result["success"]:
                extraction_times.append(result["duration"])
        
        # æµ‹è¯•æœ¬åœ°æ•°æ®åº“æœç´¢æ€§èƒ½
        local_search_times = []
        test_companies = ["æµ·å°”é›†å›¢", "åä¸º", "è…¾è®¯", "é˜¿é‡Œå·´å·´", "ç™¾åº¦"]
        
        for company in test_companies:
            result = self.measure_response_time(
                enterprise_service.search_local_database,
                company
            )
            if result["success"]:
                local_search_times.append(result["duration"])
        
        metrics = {
            "name_extraction": {
                "avg_time": statistics.mean(extraction_times) if extraction_times else 0,
                "min_time": min(extraction_times) if extraction_times else 0,
                "max_time": max(extraction_times) if extraction_times else 0,
                "success_rate": len(extraction_times) / len(test_inputs) * 100
            },
            "local_search": {
                "avg_time": statistics.mean(local_search_times) if local_search_times else 0,
                "min_time": min(local_search_times) if local_search_times else 0,
                "max_time": max(local_search_times) if local_search_times else 0,
                "success_rate": len(local_search_times) / len(test_companies) * 100
            }
        }
        
        self.log_result("ä¸šåŠ¡æœåŠ¡æ€§èƒ½", metrics)
        return metrics
    
    def test_concurrent_performance(self):
        """æµ‹è¯•å¹¶å‘æ€§èƒ½"""
        print("âš¡ æµ‹è¯•å¹¶å‘æ€§èƒ½...")
        
        def worker_task(task_id):
            """å·¥ä½œçº¿ç¨‹ä»»åŠ¡"""
            start = time.time()
            try:
                # æ¨¡æ‹Ÿå¹¶å‘æŸ¥è¯¢
                customers = get_all_customers()
                enterprises = get_all_enterprises()
                
                # æ¨¡æ‹Ÿä¸šåŠ¡å¤„ç†
                enterprise_service = EnterpriseService()
                result = enterprise_service.search_service.extract_company_name_from_input(
                    f"æŸ¥è¯¢ä»»åŠ¡{task_id}çš„ä¼ä¸šä¿¡æ¯"
                )
                
                duration = time.time() - start
                return {"task_id": task_id, "success": True, "duration": duration}
            except Exception as e:
                duration = time.time() - start
                return {"task_id": task_id, "success": False, "duration": duration, "error": str(e)}
        
        # æµ‹è¯•ä¸åŒå¹¶å‘çº§åˆ«
        concurrency_levels = [1, 5, 10, 20]
        concurrent_results = {}
        
        for level in concurrency_levels:
            print(f"  æµ‹è¯•å¹¶å‘çº§åˆ«: {level}")
            start_time = time.time()
            
            with ThreadPoolExecutor(max_workers=level) as executor:
                futures = [executor.submit(worker_task, i) for i in range(level)]
                results = [future.result() for future in as_completed(futures)]
            
            total_time = time.time() - start_time
            successful_tasks = [r for r in results if r["success"]]
            failed_tasks = [r for r in results if not r["success"]]
            
            if successful_tasks:
                task_times = [r["duration"] for r in successful_tasks]
                concurrent_results[f"level_{level}"] = {
                    "total_time": total_time,
                    "avg_task_time": statistics.mean(task_times),
                    "min_task_time": min(task_times),
                    "max_task_time": max(task_times),
                    "success_rate": len(successful_tasks) / level * 100,
                    "throughput": level / total_time,  # ä»»åŠ¡/ç§’
                    "failed_count": len(failed_tasks)
                }
            else:
                concurrent_results[f"level_{level}"] = {
                    "total_time": total_time,
                    "success_rate": 0,
                    "failed_count": len(failed_tasks),
                    "error": "æ‰€æœ‰ä»»åŠ¡éƒ½å¤±è´¥äº†"
                }
        
        self.log_result("å¹¶å‘æ€§èƒ½", concurrent_results)
        return concurrent_results
    
    def test_memory_usage(self):
        """æµ‹è¯•å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        print("ğŸ’¾ æµ‹è¯•å†…å­˜ä½¿ç”¨æƒ…å†µ...")
        
        process = psutil.Process()
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        # æ‰§è¡Œä¸€ç³»åˆ—æ“ä½œå¹¶ç›‘æ§å†…å­˜
        memory_samples = [initial_memory]
        
        # å¤§é‡æ•°æ®æŸ¥è¯¢
        for i in range(10):
            customers = get_all_customers()
            enterprises = get_all_enterprises()
            current_memory = process.memory_info().rss / 1024 / 1024
            memory_samples.append(current_memory)
            time.sleep(0.1)
        
        # ä¸šåŠ¡æœåŠ¡æ“ä½œ
        enterprise_service = EnterpriseService()
        for i in range(5):
            result = enterprise_service.search_service.extract_company_name_from_input(
                f"æµ‹è¯•å†…å­˜ä½¿ç”¨æƒ…å†µ {i}"
            )
            current_memory = process.memory_info().rss / 1024 / 1024
            memory_samples.append(current_memory)
            time.sleep(0.1)
        
        final_memory = process.memory_info().rss / 1024 / 1024
        
        metrics = {
            "initial_memory_mb": initial_memory,
            "final_memory_mb": final_memory,
            "peak_memory_mb": max(memory_samples),
            "memory_increase_mb": final_memory - initial_memory,
            "avg_memory_mb": statistics.mean(memory_samples),
            "memory_samples": len(memory_samples)
        }
        
        self.log_result("å†…å­˜ä½¿ç”¨", metrics)
        return metrics
    
    def test_system_resources(self):
        """æµ‹è¯•ç³»ç»Ÿèµ„æºä½¿ç”¨"""
        print("ğŸ–¥ï¸ æµ‹è¯•ç³»ç»Ÿèµ„æºä½¿ç”¨...")
        
        # CPUä½¿ç”¨ç‡
        cpu_percent_before = psutil.cpu_percent(interval=1)
        
        # æ‰§è¡Œä¸€äº›CPUå¯†é›†å‹æ“ä½œ
        start_time = time.time()
        for i in range(100):
            customers = get_all_customers()
            # æ¨¡æ‹Ÿä¸€äº›è®¡ç®—
            _ = sum(range(1000))
        
        cpu_percent_after = psutil.cpu_percent(interval=1)
        execution_time = time.time() - start_time
        
        # å†…å­˜ä¿¡æ¯
        memory = psutil.virtual_memory()
        
        # ç£ç›˜ä¿¡æ¯
        disk = psutil.disk_usage('/')
        
        metrics = {
            "cpu_usage": {
                "before_percent": cpu_percent_before,
                "after_percent": cpu_percent_after,
                "increase_percent": cpu_percent_after - cpu_percent_before
            },
            "memory": {
                "total_gb": memory.total / 1024 / 1024 / 1024,
                "available_gb": memory.available / 1024 / 1024 / 1024,
                "used_percent": memory.percent
            },
            "disk": {
                "total_gb": disk.total / 1024 / 1024 / 1024,
                "free_gb": disk.free / 1024 / 1024 / 1024,
                "used_percent": (disk.used / disk.total) * 100
            },
            "execution_time": execution_time
        }
        
        self.log_result("ç³»ç»Ÿèµ„æº", metrics)
        return metrics
    
    def generate_performance_report(self):
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        total_time = time.time() - self.start_time
        
        # è®¡ç®—ç»¼åˆè¯„åˆ†
        scores = {}
        
        # æ•°æ®åº“æ€§èƒ½è¯„åˆ† (åŸºäºå¹³å‡å“åº”æ—¶é—´)
        if "æ•°æ®åº“æ€§èƒ½" in self.results:
            db_perf = self.results["æ•°æ®åº“æ€§èƒ½"]
            avg_time = db_perf.get("single_query", {}).get("avg_time", 1)
            # å“åº”æ—¶é—´è¶ŠçŸ­åˆ†æ•°è¶Šé«˜ (æ»¡åˆ†100ï¼Œ0.1ç§’ä»¥ä¸‹ä¸ºæ»¡åˆ†)
            db_score = max(0, min(100, (0.1 - avg_time) / 0.1 * 100))
            scores["æ•°æ®åº“æ€§èƒ½"] = db_score
        
        # ä¸šåŠ¡æœåŠ¡æ€§èƒ½è¯„åˆ†
        if "ä¸šåŠ¡æœåŠ¡æ€§èƒ½" in self.results:
            service_perf = self.results["ä¸šåŠ¡æœåŠ¡æ€§èƒ½"]
            avg_time = service_perf.get("name_extraction", {}).get("avg_time", 1)
            service_score = max(0, min(100, (0.5 - avg_time) / 0.5 * 100))
            scores["ä¸šåŠ¡æœåŠ¡æ€§èƒ½"] = service_score
        
        # å¹¶å‘æ€§èƒ½è¯„åˆ†
        if "å¹¶å‘æ€§èƒ½" in self.results:
            concurrent_perf = self.results["å¹¶å‘æ€§èƒ½"]
            # åŸºäºæœ€é«˜å¹¶å‘çº§åˆ«çš„æˆåŠŸç‡
            max_level_key = max([k for k in concurrent_perf.keys() if k.startswith("level_")])
            success_rate = concurrent_perf[max_level_key].get("success_rate", 0)
            concurrent_score = success_rate
            scores["å¹¶å‘æ€§èƒ½"] = concurrent_score
        
        # ç»¼åˆè¯„åˆ†
        overall_score = statistics.mean(scores.values()) if scores else 0
        
        report = {
            "performance_test": {
                "timestamp": datetime.now().isoformat(),
                "total_duration": f"{total_time:.3f}s",
                "overall_score": f"{overall_score:.1f}/100"
            },
            "scores": scores,
            "detailed_results": self.results,
            "recommendations": self.generate_recommendations()
        }
        
        return report
    
    def generate_recommendations(self):
        """ç”Ÿæˆæ€§èƒ½ä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        # æ•°æ®åº“æ€§èƒ½å»ºè®®
        if "æ•°æ®åº“æ€§èƒ½" in self.results:
            db_perf = self.results["æ•°æ®åº“æ€§èƒ½"]
            avg_time = db_perf.get("single_query", {}).get("avg_time", 0)
            if avg_time > 0.5:
                recommendations.append("æ•°æ®åº“æŸ¥è¯¢å“åº”æ—¶é—´è¾ƒæ…¢ï¼Œå»ºè®®æ·»åŠ ç´¢å¼•æˆ–ä¼˜åŒ–æŸ¥è¯¢è¯­å¥")
            if db_perf.get("single_query", {}).get("success_rate", 100) < 95:
                recommendations.append("æ•°æ®åº“æŸ¥è¯¢æˆåŠŸç‡è¾ƒä½ï¼Œå»ºè®®æ£€æŸ¥è¿æ¥æ± é…ç½®")
        
        # å¹¶å‘æ€§èƒ½å»ºè®®
        if "å¹¶å‘æ€§èƒ½" in self.results:
            concurrent_perf = self.results["å¹¶å‘æ€§èƒ½"]
            for level_key, metrics in concurrent_perf.items():
                if isinstance(metrics, dict) and metrics.get("success_rate", 100) < 90:
                    recommendations.append(f"é«˜å¹¶å‘åœºæ™¯ä¸‹æˆåŠŸç‡è¾ƒä½ï¼Œå»ºè®®ä¼˜åŒ–èµ„æºç®¡ç†å’Œé”™è¯¯å¤„ç†")
                    break
        
        # å†…å­˜ä½¿ç”¨å»ºè®®
        if "å†…å­˜ä½¿ç”¨" in self.results:
            memory_metrics = self.results["å†…å­˜ä½¿ç”¨"]
            memory_increase = memory_metrics.get("memory_increase_mb", 0)
            if memory_increase > 100:
                recommendations.append("å†…å­˜ä½¿ç”¨å¢é•¿è¾ƒå¤§ï¼Œå»ºè®®æ£€æŸ¥æ˜¯å¦å­˜åœ¨å†…å­˜æ³„æ¼")
        
        if not recommendations:
            recommendations.append("ç³»ç»Ÿæ€§èƒ½è¡¨ç°è‰¯å¥½ï¼Œæ— éœ€ç‰¹åˆ«ä¼˜åŒ–")
        
        return recommendations
    
    def run_all_benchmarks(self):
        """è¿è¡Œæ‰€æœ‰æ€§èƒ½æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹æ€§èƒ½åŸºå‡†æµ‹è¯•...")
        print("=" * 60)
        
        try:
            # è¿è¡Œå„é¡¹æµ‹è¯•
            self.test_database_performance()
            self.test_business_service_performance()
            self.test_concurrent_performance()
            self.test_memory_usage()
            self.test_system_resources()
            
            # ç”ŸæˆæŠ¥å‘Š
            report = self.generate_performance_report()
            
            # ä¿å­˜æŠ¥å‘Š
            report_file = f"logs/performance_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            os.makedirs("logs", exist_ok=True)
            
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            
            print("=" * 60)
            print("ğŸ“Š æ€§èƒ½æµ‹è¯•ç»“æœæ±‡æ€»:")
            print(f"   æ€»è€—æ—¶: {report['performance_test']['total_duration']}")
            print(f"   ç»¼åˆè¯„åˆ†: {report['performance_test']['overall_score']}")
            print()
            print("ğŸ“ˆ å„é¡¹æ€§èƒ½è¯„åˆ†:")
            for metric, score in report['scores'].items():
                print(f"   {metric}: {score:.1f}/100")
            print()
            print("ğŸ’¡ ä¼˜åŒ–å»ºè®®:")
            for i, rec in enumerate(report['recommendations'], 1):
                print(f"   {i}. {rec}")
            print()
            print(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
            
            return report
            
        except Exception as e:
            print(f"âŒ æ€§èƒ½æµ‹è¯•å¼‚å¸¸: {str(e)}")
            return None


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ“Š Phase 6.2: æ€§èƒ½åŸºå‡†æµ‹è¯•")
    print("è¯„ä¼°ç³»ç»Ÿå“åº”æ—¶é—´ã€å¹¶å‘èƒ½åŠ›å’Œèµ„æºä½¿ç”¨æƒ…å†µ")
    print()
    
    benchmark = PerformanceBenchmark()
    report = benchmark.run_all_benchmarks()
    
    if report:
        overall_score = float(report['performance_test']['overall_score'].split('/')[0])
        if overall_score >= 80:
            print("\nğŸ‰ ç³»ç»Ÿæ€§èƒ½è¡¨ç°ä¼˜ç§€ï¼")
            return 0
        elif overall_score >= 60:
            print("\nâœ… ç³»ç»Ÿæ€§èƒ½è¡¨ç°è‰¯å¥½ï¼Œæœ‰ä¼˜åŒ–ç©ºé—´ã€‚")
            return 0
        else:
            print("\nâš ï¸ ç³»ç»Ÿæ€§èƒ½éœ€è¦ä¼˜åŒ–ï¼Œè¯·å‚è€ƒå»ºè®®è¿›è¡Œæ”¹è¿›ã€‚")
            return 1
    else:
        print("\nâŒ æ€§èƒ½æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç³»ç»Ÿé…ç½®ã€‚")
        return 1


if __name__ == "__main__":
    exit(main())