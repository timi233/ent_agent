"""
Phase 6.2: ç®€åŒ–æ€§èƒ½åŸºå‡†æµ‹è¯•
è¯„ä¼°ç³»ç»Ÿå“åº”æ—¶é—´ã€å¹¶å‘èƒ½åŠ›å’Œèµ„æºä½¿ç”¨æƒ…å†µ
"""

import os
import sys
import time
import json
import statistics
from datetime import datetime
from typing import Dict, Any, List
from concurrent.futures import ThreadPoolExecutor, as_completed
import psutil

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(__file__))

from config.simple_settings import load_settings
from domain.services.enterprise_service import EnterpriseService


class SimplePerformanceBenchmark:
    """ç®€åŒ–æ€§èƒ½åŸºå‡†æµ‹è¯•å™¨"""
    
    def __init__(self):
        self.settings = load_settings()
        self.results = {}
        self.start_time = time.time()
        
    def log_result(self, test_name: str, metrics: Dict[str, Any]):
        """è®°å½•æµ‹è¯•ç»“æœ"""
        self.results[test_name] = {
            **metrics,
            "timestamp": datetime.now().isoformat()
        }
        print(f"ğŸ“Š {test_name}: {metrics}")
    
    def measure_response_time(self, func, *args, **kwargs):
        """æµ‹é‡å‡½æ•°å“åº”æ—¶é—´"""
        start = time.time()
        try:
            result = func(*args, **kwargs)
            duration = time.time() - start
            return {"success": True, "duration": duration, "result": result}
        except Exception as e:
            duration = time.time() - start
            return {"success": False, "duration": duration, "error": str(e)}
    
    def test_business_service_performance(self):
        """æµ‹è¯•ä¸šåŠ¡æœåŠ¡æ€§èƒ½"""
        print("ğŸ§  æµ‹è¯•ä¸šåŠ¡æœåŠ¡æ€§èƒ½...")
        
        try:
            enterprise_service = EnterpriseService()
            
            # æµ‹è¯•ä¼ä¸šåç§°æå–æ€§èƒ½
            extraction_times = []
            test_inputs = [
                "æŸ¥è¯¢æµ·å°”é›†å›¢çš„ä¿¡æ¯",
                "åä¸ºæŠ€æœ¯æœ‰é™å…¬å¸æ€ä¹ˆæ ·",
                "è…¾è®¯æ§è‚¡çš„è¯¦ç»†èµ„æ–™",
                "é˜¿é‡Œå·´å·´é›†å›¢çš„ä¸šåŠ¡èŒƒå›´",
                "ç™¾åº¦å…¬å¸çš„å‘å±•å†ç¨‹"
            ]
            
            for input_text in test_inputs:
                result = self.measure_response_time(
                    enterprise_service.search_service.extract_company_name_from_input,
                    input_text
                )
                if result["success"]:
                    extraction_times.append(result["duration"])
            
            # æµ‹è¯•æœ¬åœ°æ•°æ®åº“æœç´¢æ€§èƒ½
            local_search_times = []
            test_companies = ["æµ·å°”é›†å›¢", "åä¸º", "è…¾è®¯", "é˜¿é‡Œå·´å·´", "ç™¾åº¦"]
            
            for company in test_companies:
                result = self.measure_response_time(
                    enterprise_service.search_local_database,
                    company
                )
                if result["success"]:
                    local_search_times.append(result["duration"])
            
            metrics = {
                "name_extraction": {
                    "avg_time": statistics.mean(extraction_times) if extraction_times else 0,
                    "min_time": min(extraction_times) if extraction_times else 0,
                    "max_time": max(extraction_times) if extraction_times else 0,
                    "success_rate": len(extraction_times) / len(test_inputs) * 100
                },
                "local_search": {
                    "avg_time": statistics.mean(local_search_times) if local_search_times else 0,
                    "min_time": min(local_search_times) if local_search_times else 0,
                    "max_time": max(local_search_times) if local_search_times else 0,
                    "success_rate": len(local_search_times) / len(test_companies) * 100
                }
            }
            
            self.log_result("ä¸šåŠ¡æœåŠ¡æ€§èƒ½", metrics)
            return metrics
            
        except Exception as e:
            print(f"ä¸šåŠ¡æœåŠ¡æ€§èƒ½æµ‹è¯•å¼‚å¸¸: {str(e)}")
            return {"error": str(e)}
    
    def test_concurrent_performance(self):
        """æµ‹è¯•å¹¶å‘æ€§èƒ½"""
        print("âš¡ æµ‹è¯•å¹¶å‘æ€§èƒ½...")
        
        def worker_task(task_id):
            """å·¥ä½œçº¿ç¨‹ä»»åŠ¡"""
            start = time.time()
            try:
                # æ¨¡æ‹Ÿä¸šåŠ¡å¤„ç†
                enterprise_service = EnterpriseService()
                result = enterprise_service.search_service.extract_company_name_from_input(
                    f"æŸ¥è¯¢ä»»åŠ¡{task_id}çš„ä¼ä¸šä¿¡æ¯"
                )
                
                duration = time.time() - start
                return {"task_id": task_id, "success": True, "duration": duration}
            except Exception as e:
                duration = time.time() - start
                return {"task_id": task_id, "success": False, "duration": duration, "error": str(e)}
        
        # æµ‹è¯•ä¸åŒå¹¶å‘çº§åˆ«
        concurrency_levels = [1, 5, 10]
        concurrent_results = {}
        
        for level in concurrency_levels:
            print(f"  æµ‹è¯•å¹¶å‘çº§åˆ«: {level}")
            start_time = time.time()
            
            try:
                with ThreadPoolExecutor(max_workers=level) as executor:
                    futures = [executor.submit(worker_task, i) for i in range(level)]
                    results = [future.result() for future in as_completed(futures)]
                
                total_time = time.time() - start_time
                successful_tasks = [r for r in results if r["success"]]
                failed_tasks = [r for r in results if not r["success"]]
                
                if successful_tasks:
                    task_times = [r["duration"] for r in successful_tasks]
                    concurrent_results[f"level_{level}"] = {
                        "total_time": total_time,
                        "avg_task_time": statistics.mean(task_times),
                        "min_task_time": min(task_times),
                        "max_task_time": max(task_times),
                        "success_rate": len(successful_tasks) / level * 100,
                        "throughput": level / total_time,  # ä»»åŠ¡/ç§’
                        "failed_count": len(failed_tasks)
                    }
                else:
                    concurrent_results[f"level_{level}"] = {
                        "total_time": total_time,
                        "success_rate": 0,
                        "failed_count": len(failed_tasks),
                        "error": "æ‰€æœ‰ä»»åŠ¡éƒ½å¤±è´¥äº†"
                    }
            except Exception as e:
                concurrent_results[f"level_{level}"] = {
                    "error": str(e)
                }
        
        self.log_result("å¹¶å‘æ€§èƒ½", concurrent_results)
        return concurrent_results
    
    def test_memory_usage(self):
        """æµ‹è¯•å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        print("ğŸ’¾ æµ‹è¯•å†…å­˜ä½¿ç”¨æƒ…å†µ...")
        
        try:
            process = psutil.Process()
            initial_memory = process.memory_info().rss / 1024 / 1024  # MB
            
            # æ‰§è¡Œä¸€ç³»åˆ—æ“ä½œå¹¶ç›‘æ§å†…å­˜
            memory_samples = [initial_memory]
            
            # ä¸šåŠ¡æœåŠ¡æ“ä½œ
            enterprise_service = EnterpriseService()
            for i in range(5):
                result = enterprise_service.search_service.extract_company_name_from_input(
                    f"æµ‹è¯•å†…å­˜ä½¿ç”¨æƒ…å†µ {i}"
                )
                current_memory = process.memory_info().rss / 1024 / 1024
                memory_samples.append(current_memory)
                time.sleep(0.1)
            
            final_memory = process.memory_info().rss / 1024 / 1024
            
            metrics = {
                "initial_memory_mb": initial_memory,
                "final_memory_mb": final_memory,
                "peak_memory_mb": max(memory_samples),
                "memory_increase_mb": final_memory - initial_memory,
                "avg_memory_mb": statistics.mean(memory_samples),
                "memory_samples": len(memory_samples)
            }
            
            self.log_result("å†…å­˜ä½¿ç”¨", metrics)
            return metrics
            
        except Exception as e:
            print(f"å†…å­˜ä½¿ç”¨æµ‹è¯•å¼‚å¸¸: {str(e)}")
            return {"error": str(e)}
    
    def test_system_resources(self):
        """æµ‹è¯•ç³»ç»Ÿèµ„æºä½¿ç”¨"""
        print("ğŸ–¥ï¸ æµ‹è¯•ç³»ç»Ÿèµ„æºä½¿ç”¨...")
        
        try:
            # CPUä½¿ç”¨ç‡
            cpu_percent_before = psutil.cpu_percent(interval=1)
            
            # æ‰§è¡Œä¸€äº›æ“ä½œ
            start_time = time.time()
            enterprise_service = EnterpriseService()
            for i in range(10):
                result = enterprise_service.search_service.extract_company_name_from_input(
                    f"æµ‹è¯•CPUä½¿ç”¨æƒ…å†µ {i}"
                )
                # æ¨¡æ‹Ÿä¸€äº›è®¡ç®—
                _ = sum(range(1000))
            
            cpu_percent_after = psutil.cpu_percent(interval=1)
            execution_time = time.time() - start_time
            
            # å†…å­˜ä¿¡æ¯
            memory = psutil.virtual_memory()
            
            # ç£ç›˜ä¿¡æ¯
            disk = psutil.disk_usage('/')
            
            metrics = {
                "cpu_usage": {
                    "before_percent": cpu_percent_before,
                    "after_percent": cpu_percent_after,
                    "increase_percent": cpu_percent_after - cpu_percent_before
                },
                "memory": {
                    "total_gb": memory.total / 1024 / 1024 / 1024,
                    "available_gb": memory.available / 1024 / 1024 / 1024,
                    "used_percent": memory.percent
                },
                "disk": {
                    "total_gb": disk.total / 1024 / 1024 / 1024,
                    "free_gb": disk.free / 1024 / 1024 / 1024,
                    "used_percent": (disk.used / disk.total) * 100
                },
                "execution_time": execution_time
            }
            
            self.log_result("ç³»ç»Ÿèµ„æº", metrics)
            return metrics
            
        except Exception as e:
            print(f"ç³»ç»Ÿèµ„æºæµ‹è¯•å¼‚å¸¸: {str(e)}")
            return {"error": str(e)}
    
    def generate_performance_report(self):
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        total_time = time.time() - self.start_time
        
        # è®¡ç®—ç»¼åˆè¯„åˆ†
        scores = {}
        
        # ä¸šåŠ¡æœåŠ¡æ€§èƒ½è¯„åˆ†
        if "ä¸šåŠ¡æœåŠ¡æ€§èƒ½" in self.results and "error" not in self.results["ä¸šåŠ¡æœåŠ¡æ€§èƒ½"]:
            service_perf = self.results["ä¸šåŠ¡æœåŠ¡æ€§èƒ½"]
            avg_time = service_perf.get("name_extraction", {}).get("avg_time", 1)
            service_score = max(0, min(100, (0.5 - avg_time) / 0.5 * 100))
            scores["ä¸šåŠ¡æœåŠ¡æ€§èƒ½"] = service_score
        
        # å¹¶å‘æ€§èƒ½è¯„åˆ†
        if "å¹¶å‘æ€§èƒ½" in self.results and "error" not in self.results["å¹¶å‘æ€§èƒ½"]:
            concurrent_perf = self.results["å¹¶å‘æ€§èƒ½"]
            # åŸºäºæœ€é«˜å¹¶å‘çº§åˆ«çš„æˆåŠŸç‡
            level_keys = [k for k in concurrent_perf.keys() if k.startswith("level_")]
            if level_keys:
                max_level_key = max(level_keys)
                success_rate = concurrent_perf[max_level_key].get("success_rate", 0)
                concurrent_score = success_rate
                scores["å¹¶å‘æ€§èƒ½"] = concurrent_score
        
        # ç»¼åˆè¯„åˆ†
        overall_score = statistics.mean(scores.values()) if scores else 50
        
        report = {
            "performance_test": {
                "timestamp": datetime.now().isoformat(),
                "total_duration": f"{total_time:.3f}s",
                "overall_score": f"{overall_score:.1f}/100"
            },
            "scores": scores,
            "detailed_results": self.results,
            "recommendations": self.generate_recommendations()
        }
        
        return report
    
    def generate_recommendations(self):
        """ç”Ÿæˆæ€§èƒ½ä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
        has_errors = any("error" in result for result in self.results.values() if isinstance(result, dict))
        
        if has_errors:
            recommendations.append("éƒ¨åˆ†æµ‹è¯•å‡ºç°é”™è¯¯ï¼Œå»ºè®®æ£€æŸ¥ç³»ç»Ÿé…ç½®å’Œä¾èµ–")
        
        # ä¸šåŠ¡æœåŠ¡æ€§èƒ½å»ºè®®
        if "ä¸šåŠ¡æœåŠ¡æ€§èƒ½" in self.results and "error" not in self.results["ä¸šåŠ¡æœåŠ¡æ€§èƒ½"]:
            service_perf = self.results["ä¸šåŠ¡æœåŠ¡æ€§èƒ½"]
            avg_time = service_perf.get("name_extraction", {}).get("avg_time", 0)
            if avg_time > 0.5:
                recommendations.append("ä¼ä¸šåç§°æå–å“åº”æ—¶é—´è¾ƒæ…¢ï¼Œå»ºè®®ä¼˜åŒ–ç®—æ³•")
        
        # å¹¶å‘æ€§èƒ½å»ºè®®
        if "å¹¶å‘æ€§èƒ½" in self.results and "error" not in self.results["å¹¶å‘æ€§èƒ½"]:
            concurrent_perf = self.results["å¹¶å‘æ€§èƒ½"]
            for level_key, metrics in concurrent_perf.items():
                if isinstance(metrics, dict) and metrics.get("success_rate", 100) < 90:
                    recommendations.append(f"é«˜å¹¶å‘åœºæ™¯ä¸‹æˆåŠŸç‡è¾ƒä½ï¼Œå»ºè®®ä¼˜åŒ–èµ„æºç®¡ç†")
                    break
        
        # å†…å­˜ä½¿ç”¨å»ºè®®
        if "å†…å­˜ä½¿ç”¨" in self.results and "error" not in self.results["å†…å­˜ä½¿ç”¨"]:
            memory_metrics = self.results["å†…å­˜ä½¿ç”¨"]
            memory_increase = memory_metrics.get("memory_increase_mb", 0)
            if memory_increase > 50:
                recommendations.append("å†…å­˜ä½¿ç”¨å¢é•¿è¾ƒå¤§ï¼Œå»ºè®®æ£€æŸ¥æ˜¯å¦å­˜åœ¨å†…å­˜æ³„æ¼")
        
        if not recommendations:
            recommendations.append("ç³»ç»Ÿæ€§èƒ½è¡¨ç°è‰¯å¥½ï¼Œæ— éœ€ç‰¹åˆ«ä¼˜åŒ–")
        
        return recommendations
    
    def run_all_benchmarks(self):
        """è¿è¡Œæ‰€æœ‰æ€§èƒ½æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹ç®€åŒ–æ€§èƒ½åŸºå‡†æµ‹è¯•...")
        print("=" * 60)
        
        try:
            # è¿è¡Œå„é¡¹æµ‹è¯•
            self.test_business_service_performance()
            self.test_concurrent_performance()
            self.test_memory_usage()
            self.test_system_resources()
            
            # ç”ŸæˆæŠ¥å‘Š
            report = self.generate_performance_report()
            
            # ä¿å­˜æŠ¥å‘Š
            report_file = f"logs/performance_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            os.makedirs("logs", exist_ok=True)
            
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            
            print("=" * 60)
            print("ğŸ“Š æ€§èƒ½æµ‹è¯•ç»“æœæ±‡æ€»:")
            print(f"   æ€»è€—æ—¶: {report['performance_test']['total_duration']}")
            print(f"   ç»¼åˆè¯„åˆ†: {report['performance_test']['overall_score']}")
            print()
            print("ğŸ“ˆ å„é¡¹æ€§èƒ½è¯„åˆ†:")
            for metric, score in report['scores'].items():
                print(f"   {metric}: {score:.1f}/100")
            print()
            print("ğŸ’¡ ä¼˜åŒ–å»ºè®®:")
            for i, rec in enumerate(report['recommendations'], 1):
                print(f"   {i}. {rec}")
            print()
            print(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
            
            return report
            
        except Exception as e:
            print(f"âŒ æ€§èƒ½æµ‹è¯•å¼‚å¸¸: {str(e)}")
            return None


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ“Š Phase 6.2: ç®€åŒ–æ€§èƒ½åŸºå‡†æµ‹è¯•")
    print("è¯„ä¼°ç³»ç»Ÿå“åº”æ—¶é—´ã€å¹¶å‘èƒ½åŠ›å’Œèµ„æºä½¿ç”¨æƒ…å†µ")
    print()
    
    benchmark = SimplePerformanceBenchmark()
    report = benchmark.run_all_benchmarks()
    
    if report:
        overall_score = float(report['performance_test']['overall_score'].split('/')[0])
        if overall_score >= 70:
            print("\nğŸ‰ ç³»ç»Ÿæ€§èƒ½è¡¨ç°è‰¯å¥½ï¼")
            return 0
        elif overall_score >= 50:
            print("\nâœ… ç³»ç»Ÿæ€§èƒ½åŸºæœ¬æ»¡è¶³è¦æ±‚ï¼Œæœ‰ä¼˜åŒ–ç©ºé—´ã€‚")
            return 0
        else:
            print("\nâš ï¸ ç³»ç»Ÿæ€§èƒ½éœ€è¦ä¼˜åŒ–ï¼Œè¯·å‚è€ƒå»ºè®®è¿›è¡Œæ”¹è¿›ã€‚")
            return 1
    else:
        print("\nâŒ æ€§èƒ½æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç³»ç»Ÿé…ç½®ã€‚")
        return 1


if __name__ == "__main__":
    exit(main())